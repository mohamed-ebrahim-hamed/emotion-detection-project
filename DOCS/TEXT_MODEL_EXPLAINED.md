# Ø´Ø±Ø­ ØªÙØµÙŠÙ„ÙŠ Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Øµ (text-model.ipynb)
# Detailed Explanation of Text Model

---

## ğŸ“‹ Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø© / Overview

Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙŠØ´Ø±Ø­ ÙƒÙ„ Ø®Ù„ÙŠØ© (Cell) ÙÙŠ Ø¯ÙØªØ± Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª `text-model.ipynb` Ø¨Ø§Ù„ØªÙØµÙŠÙ„.
ÙŠØªØ¶Ù…Ù† Ø§Ù„Ø¯ÙØªØ± ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ DistilBERT (Transformer) Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ 28 Ø¹Ø§Ø·ÙØ© Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ.

This guide explains every cell in the `text-model.ipynb` notebook in detail.
The notebook includes training a DistilBERT (Transformer) model to recognize 28 different emotions from text.

---

## ğŸ“Š Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª / Dataset Information

### GoEmotions Dataset:
- **Ø§Ù„Ù…ØµØ¯Ø±**: Google Research
- **Ø§Ù„Ø­Ø¬Ù…**: 58,000 ØªØ¹Ù„ÙŠÙ‚ Ù…Ù† Reddit
- **Ø§Ù„Ø¹ÙˆØ§Ø·Ù**: 28 Ø¹Ø§Ø·ÙØ© Ù…Ø®ØªÙ„ÙØ©
- **Ø§Ù„Ù„ØºØ©**: Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
- **Ø§Ù„Ù†ÙˆØ¹**: Multi-label classification (ÙŠÙ…ÙƒÙ† Ù„Ù„Ù†Øµ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£ÙƒØ«Ø± Ù…Ù† Ø¹Ø§Ø·ÙØ©)

### Ø§Ù„Ø¹ÙˆØ§Ø·Ù Ø§Ù„Ù€ 28 / The 28 Emotions:
1. admiration (Ø¥Ø¹Ø¬Ø§Ø¨)
2. amusement (ØªØ³Ù„ÙŠØ©)
3. anger (ØºØ¶Ø¨)
4. annoyance (Ø§Ù†Ø²Ø¹Ø§Ø¬)
5. approval (Ù…ÙˆØ§ÙÙ‚Ø©)
6. caring (Ø§Ù‡ØªÙ…Ø§Ù…)
7. confusion (Ø§Ø±ØªØ¨Ø§Ùƒ)
8. curiosity (ÙØ¶ÙˆÙ„)
9. desire (Ø±ØºØ¨Ø©)
10. disappointment (Ø®ÙŠØ¨Ø© Ø£Ù…Ù„)
11. disapproval (Ø±ÙØ¶)
12. disgust (Ø§Ø´Ù…Ø¦Ø²Ø§Ø²)
13. embarrassment (Ø¥Ø­Ø±Ø§Ø¬)
14. excitement (Ø­Ù…Ø§Ø³)
15. fear (Ø®ÙˆÙ)
16. gratitude (Ø§Ù…ØªÙ†Ø§Ù†)
17. grief (Ø­Ø²Ù† Ø´Ø¯ÙŠØ¯)
18. joy (ÙØ±Ø­)
19. love (Ø­Ø¨)
20. nervousness (ØªÙˆØªØ±)
21. neutral (Ù…Ø­Ø§ÙŠØ¯)
22. optimism (ØªÙØ§Ø¤Ù„)
23. pride (ÙØ®Ø±)
24. realization (Ø¥Ø¯Ø±Ø§Ùƒ)
25. relief (Ø§Ø±ØªÙŠØ§Ø­)
26. remorse (Ù†Ø¯Ù…)
27. sadness (Ø­Ø²Ù†)
28. surprise (Ù…ÙØ§Ø¬Ø£Ø©)

---

## ğŸ“¦ Cell 1: Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© / Importing Basic Libraries

```python
import numpy as np 
import pandas as pd 

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
```

**Ø§Ù„Ø´Ø±Ø­ / Explanation:**
- **numpy**: Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© ÙˆØ§Ù„Ù…ØµÙÙˆÙØ§Øª
- **pandas**: Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø¬Ø¯Ø§ÙˆÙ„
- **os.walk()**: Ù„Ø§Ø³ØªØ¹Ø±Ø§Ø¶ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
- Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ ÙŠØ·Ø¨Ø¹ Ù…Ø³Ø§Ø±Ø§Øª Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©

**Ø§Ù„Ù‡Ø¯Ù**: Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª

---

## ğŸ”§ Cell 2: ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© / Installing Required Libraries

```python
!pip install transformers==4.30.2 -q
```

**Ø§Ù„Ø´Ø±Ø­ / Explanation:**
- **transformers**: Ù…ÙƒØªØ¨Ø© Hugging Face Ù„Ù„Ø¹Ù…Ù„ Ù…Ø¹ Ù†Ù…Ø§Ø°Ø¬ Transformer
- **4.30.2**: Ø¥ØµØ¯Ø§Ø± Ù…Ø­Ø¯Ø¯ Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªÙˆØ§ÙÙ‚
- **-q**: ÙˆØ¶Ø¹ Ù‡Ø§Ø¯Ø¦ (quiet mode) Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª

**Ù…Ø§ Ù‡ÙŠ TransformersØŸ**
- Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ø«ÙˆØ±ÙŠØ© ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© (NLP)
- ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø¢Ù„ÙŠØ© Attention
- Ø£Ø³Ø§Ø³ Ù†Ù…Ø§Ø°Ø¬ Ù…Ø«Ù„ BERTØŒ GPTØŒ T5

**Ù„Ù…Ø§Ø°Ø§ DistilBERTØŸ**
- Ù†Ø³Ø®Ø© Ù…ØµØºØ±Ø© Ù…Ù† BERT
- 40% Ø£Ø³Ø±Ø¹
- 60% Ø£ØµØºØ± ÙÙŠ Ø§Ù„Ø­Ø¬Ù…
- 97% Ù…Ù† Ø£Ø¯Ø§Ø¡ BERT Ø§Ù„Ø£ØµÙ„ÙŠ

---

## ğŸ“š Cell 3: Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù…ÙƒØªØ¨Ø§Øª Deep Learning / Importing Deep Learning Libraries

```python
import pandas as pd
import numpy as np
import torch

from torch.utils.data import TensorDataset, DataLoader
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification
```

**Ø§Ù„Ø´Ø±Ø­ / Explanation:**

### PyTorch Components:
- **torch**: Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ Deep Learning Ù…Ù† Facebook
- **TensorDataset**: Ù„ØªØºÙ„ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ dataset
- **DataLoader**: Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ batches

### Transformers Components:
- **DistilBertTokenizerFast**: 
  - ÙŠØ­ÙˆÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… (tokens)
  - Ù†Ø³Ø®Ø© Ø³Ø±ÙŠØ¹Ø© Ù…ÙƒØªÙˆØ¨Ø© Ø¨Ù„ØºØ© Rust
  - ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ±Ø¹ÙŠØ© (subwords)

- **DistilBertForSequenceClassification**:
  - Ù†Ù…ÙˆØ°Ø¬ DistilBERT Ù…Ø¬Ù‡Ø² Ù„Ù„ØªØµÙ†ÙŠÙ
  - ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ DistilBERT + Ø·Ø¨Ù‚Ø© classification

**Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† TensorFlow Ùˆ PyTorch:**
| TensorFlow | PyTorch |
|------------|---------|
| Ù…Ù† Google | Ù…Ù† Facebook |
| Static graphs | Dynamic graphs |
| Production focus | Research focus |
| Ø§Ø³ØªØ®Ø¯Ù…Ù†Ø§Ù‡ Ù„Ù„ØµÙˆØª | Ù†Ø³ØªØ®Ø¯Ù…Ù‡ Ù„Ù„Ù†Øµ |

---

## ğŸ“‚ Cell 4: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª / Loading Data

```python
DATA_DIR = "/kaggle/input/goemotions/data"

train_df = pd.read_csv(f"{DATA_DIR}/train.tsv", sep="\t", header=None, 
                       names=["text","labels","id"])
dev_df   = pd.read_csv(f"{DATA_DIR}/dev.tsv", sep="\t", header=None, 
                       names=["text","labels","id"])
test_df  = pd.read_csv(f"{DATA_DIR}/test.tsv", sep="\t", header=None, 
                       names=["text","labels","id"])

print(f"Train: {len(train_df)} samples")
print(f"Dev:   {len(dev_df)} samples")
print(f"Test:  {len(test_df)} samples")
```

**Ø§Ù„Ø´Ø±Ø­ / Explanation:**

### Ø¨Ù†ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª / Data Structure:
Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù…Ù„ÙØ§Øª TSV (Tab-Separated Values):

```
text                                    labels      id
I love this movie!                      17,18       123
This is confusing and scary             6,14        124
Great job, well done!                   0,4         125
```

### Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© / Columns:
1. **text**: Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØµÙ†ÙŠÙÙ‡
2. **labels**: Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¹ÙˆØ§Ø·Ù Ù…ÙØµÙˆÙ„Ø© Ø¨ÙÙˆØ§ØµÙ„ (comma-separated)
   - Ù…Ø«Ø§Ù„: "0,17,21" = admiration + joy + optimism
3. **id**: Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯ Ù„ÙƒÙ„ Ù†Øµ

### Ø§Ù„ØªÙ‚Ø³ÙŠÙ… / Split:
- **train_df**: Ù„Ù„ØªØ¯Ø±ÙŠØ¨ (~80% Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)
- **dev_df**: Ù„Ù„ØªØ­Ù‚Ù‚ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (~10%)
- **test_df**: Ù„Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (~10%)

**Ù…Ù„Ø§Ø­Ø¸Ø© Ù…Ù‡Ù…Ø©**: 
- **Multi-label classification**: Ø§Ù„Ù†Øµ Ø§Ù„ÙˆØ§Ø­Ø¯ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£ÙƒØ«Ø± Ù…Ù† Ø¹Ø§Ø·ÙØ©
- Ù…Ø«Ø§Ù„: "I'm so happy but also surprised!" â†’ happy + surprise

---

## ğŸ”¢ Cell 5: ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø¥Ù„Ù‰ Multi-hot Encoding / Converting Labels

```python
NUM_LABELS = 28

def to_multihot(label_str):
    indices = list(map(int, label_str.split(",")))
    arr = np.zeros(NUM_LABELS)
    arr[indices] = 1
    return arr

train_df["multihot"] = train_df["labels"].apply(to_multihot)
dev_df["multihot"]   = dev_df["labels"].apply(to_multihot)
test_df["multihot"]  = test_df["labels"].apply(to_multihot)

print("Example:")
print(f"Original: {train_df.iloc[0]['labels']}")
print(f"Multi-hot: {train_df.iloc[0]['multihot']}")
```

**Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ / Detailed Explanation:**

### Ù…Ø§ Ù‡Ùˆ Multi-hot EncodingØŸ
ØªØ­ÙˆÙŠÙ„ Ù…Ù† Ù†Øµ Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ© binary:

```
Ù‚Ø¨Ù„ (Before):          Ø¨Ø¹Ø¯ (After):
"0,17,21"      â†’    [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0]
                     â†‘                                 â†‘           â†‘
                   emotion 0                      emotion 17   emotion 21
```

### Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø¯Ø§Ù„Ø© / Function Steps:

1. **split(",")**: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø©
   ```python
   "0,17,21" â†’ ["0", "17", "21"]
   ```

2. **map(int, ...)**: ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…
   ```python
   ["0", "17", "21"] â†’ [0, 17, 21]
   ```

3. **np.zeros(28)**: Ø¥Ù†Ø´Ø§Ø¡ Ù…ØµÙÙˆÙØ© Ø£ØµÙØ§Ø±
   ```python
   [0, 0, 0, 0, ..., 0]  # 28 Ø¹Ù†ØµØ±
   ```

4. **arr[indices] = 1**: ØªÙØ¹ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
   ```python
   arr[0] = 1
   arr[17] = 1
   arr[21] = 1
   ```

### Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† One-hot Ùˆ Multi-hot:

**One-hot** (Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØµÙˆØª):
```
ÙÙ‚Ø· ÙˆØ§Ø­Ø¯ Ù…ÙØ¹Ù‘Ù„: [0, 0, 1, 0, 0, 0, 0]
                            â†‘
                    Ø¹Ø§Ø·ÙØ© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·
```

**Multi-hot** (Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Øµ):
```
Ø¹Ø¯Ø© Ù…ÙØ¹Ù‘Ù„Ø©: [1, 0, 0, 0, 1, 0, 1]
             â†‘           â†‘     â†‘
        Ø¹Ø¯Ø© Ø¹ÙˆØ§Ø·Ù ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª
```

---

## ğŸ”¤ Cell 6: Ø¥Ù†Ø´Ø§Ø¡ Tokenizer / Creating Tokenizer

```python
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

def encode(batch):
    return tokenizer(
        batch["text"].tolist(),
        truncation=True,
        padding="max_length",
        max_length=128
    )
```

**Ø§Ù„Ø´Ø±Ø­ / Explanation:**

### Ù…Ø§ Ù‡Ùˆ TokenizerØŸ
ÙŠØ­ÙˆÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… ÙŠÙÙ‡Ù…Ù‡Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:

```
Input Text:
"I love this movie!"

â†“ Tokenization â†“

Tokens:
["i", "love", "this", "movie", "!"]

â†“ Convert to IDs â†“

Token IDs:
[1045, 2293, 2023, 3185, 999]

â†“ Add Special Tokens â†“

Final IDs:
[101, 1045, 2293, 2023, 3185, 999, 102]
  â†‘                                  â†‘
[CLS]                              [SEP]
```

### Special Tokens:
- **[CLS]** (101): Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù†Øµ (Classification token)
- **[SEP]** (102): Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù†Øµ (Separator token)
- **[PAD]** (0): Ù„Ù„ØªØ¹Ø¨Ø¦Ø© (Padding)

### Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª / Parameters:

**truncation=True**:
```
Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ Ø£Ø·ÙˆÙ„ Ù…Ù† 128 ÙƒÙ„Ù…Ø©:
"This is a very very ... very long text"
                            â†“
"This is a very very ... [Ù‚Øµ Ø§Ù„Ø¨Ø§Ù‚ÙŠ]"
```

**padding="max_length"**:
```
Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ Ø£Ù‚ØµØ± Ù…Ù† 128 ÙƒÙ„Ù…Ø©:
[101, 1045, 2293, 102, 0, 0, 0, ... , 0]
                       â†‘
              Ù…Ù„Ø¡ Ø¨Ø§Ù„Ø£ØµÙØ§Ø± Ø­ØªÙ‰ 128
```

**max_length=128**:
- Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ
- ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„Ø³Ø±Ø¹Ø©
- BERT Ø§Ù„Ø£ØµÙ„ÙŠ ÙŠØ¯Ø¹Ù… Ø­ØªÙ‰ 512

### Ù„Ù…Ø§Ø°Ø§ "uncased"ØŸ
- **uncased**: ÙŠØ­ÙˆÙ„ ÙƒÙ„ Ø´ÙŠØ¡ Ø¥Ù„Ù‰ lowercase
  - "Hello" â†’ "hello"
  - "HELLO" â†’ "hello"
- **cased**: ÙŠØ­ÙØ¸ Ø§Ù„Ø­Ø§Ù„Ø©
  - "Hello" ÙŠØ¨Ù‚Ù‰ "Hello"
- **uncased** Ø£Ø³Ø±Ø¹ ÙˆØºØ§Ù„Ø¨Ù‹Ø§ Ø£ÙØ¶Ù„ Ù„Ù„Ù…Ø´Ø§Ø¹Ø±

---

## ğŸ”„ Cell 7: ØªØ·Ø¨ÙŠÙ‚ Tokenization / Applying Tokenization

```python
train_enc = encode(train_df)
dev_enc   = encode(dev_df)
test_enc  = encode(test_df)
```

**Ø§Ù„Ø´Ø±Ø­ / Explanation:**
ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© encode Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:
- ØªØ­ÙˆÙŠÙ„ ÙƒÙ„ Ù†Øµ Ø¥Ù„Ù‰ token IDs
- Ø¥Ù†Ø´Ø§Ø¡ attention masks
- ØªØ·Ø¨ÙŠÙ‚ padding Ùˆ truncation

**Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª / Outputs:**
```python
{
    'input_ids': [[101, 1045, 2293, ..., 0, 0, 0],
                  [101, 2023, 2003, ..., 0, 0, 0],
                  ...],
    'attention_mask': [[1, 1, 1, ..., 0, 0, 0],
                       [1, 1, 1, ..., 0, 0, 0],
                       ...]
}
```

### Ù…Ø§ Ù‡Ùˆ Attention MaskØŸ
ÙŠØ®Ø¨Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£ÙŠÙ† ÙŠØ±ÙƒØ²:

```
input_ids:      [101, 1045, 2293, 102, 0, 0, 0, 0]
attention_mask: [  1,    1,    1,   1, 0, 0, 0, 0]
                 â†‘    â†‘    â†‘    â†‘   â†‘
              Ø§Ù†ØªØ¨Ù‡ Ù„Ù‡Ø°Ù‡    ØªØ¬Ø§Ù‡Ù„ Ù‡Ø°Ù‡ (padding)
```

---

## ğŸ“¦ Cell 8: Ø¥Ù†Ø´Ø§Ø¡ PyTorch Datasets / Creating PyTorch Datasets

```python
train_dataset = TensorDataset(
    torch.tensor(train_enc["input_ids"]),
    torch.tensor(train_enc["attention_mask"]),
    torch.tensor(np.vstack(train_df["multihot"].values), dtype=torch.float)
)

dev_dataset = TensorDataset(
    torch.tensor(dev_enc["input_ids"]),
    torch.tensor(dev_enc["attention_mask"]),
    torch.tensor(np.vstack(dev_df["multihot"].values), dtype=torch.float)
)

test_dataset = TensorDataset(
    torch.tensor(test_enc["input_ids"]),
    torch.tensor(test_enc["attention_mask"]),
    torch.tensor(np.vstack(test_df["multihot"].values), dtype=torch.float)
)
```

**Ø§Ù„Ø´Ø±Ø­ / Explanation:**

### Ù…Ø§ Ù‡Ùˆ TensorDatasetØŸ
ÙŠØ¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹Ù‹Ø§ ÙÙŠ dataset ÙˆØ§Ø­Ø¯:

```
Dataset = {
    Input IDs:       [101, 1045, 2293, ...]
    Attention Mask:  [1, 1, 1, ...]
    Labels:          [1, 0, 0, 0, 1, 0, ...]
}
```

### Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª / Components:

1. **torch.tensor(train_enc["input_ids"])**:
   - Token IDs Ù„ÙƒÙ„ Ù†Øµ
   - Shape: (n_samples, 128)

2. **torch.tensor(train_enc["attention_mask"])**:
   - Attention masks
   - Shape: (n_samples, 128)

3. **torch.tensor(..., dtype=torch.float)**:
   - Multi-hot labels
   - Shape: (n_samples, 28)
   - **dtype=torch.float**: Ù…Ù‡Ù… Ù„Ù€ BCE Loss

### np.vstack:
ØªØ­ÙˆÙŠÙ„ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† arrays Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ© ÙˆØ§Ø­Ø¯Ø©:

```python
Before:
[[1,0,0], [0,1,0], [1,1,0]]  # Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† 3 arrays

After vstack:
[[1,0,0],
 [0,1,0],
 [1,1,0]]  # Ù…ØµÙÙˆÙØ© ÙˆØ§Ø­Ø¯Ø© (3, 3)
```

---

## ğŸ”„ Cell 9: Ø¥Ù†Ø´Ø§Ø¡ DataLoaders / Creating DataLoaders

```python
batch_size = 16

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
dev_loader   = DataLoader(dev_dataset, batch_size=batch_size)
```

**Ø§Ù„Ø´Ø±Ø­ / Explanation:**

### Ù…Ø§ Ù‡Ùˆ DataLoaderØŸ
ÙŠÙ‚Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ batches ÙˆÙŠØ­Ù…Ù„Ù‡Ø§ Ø¨ÙƒÙØ§Ø¡Ø©:

```
Dataset (1000 samples)
         â†“
DataLoader (batch_size=16)
         â†“
Batch 1: [samples 1-16]
Batch 2: [samples 17-32]
Batch 3: [samples 33-48]
...
Batch 63: [samples 993-1000]  # Ø¢Ø®Ø± batch Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø£ØµØºØ±
```

### Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª / Parameters:

**batch_size=16**:
- Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª ÙÙŠ ÙƒÙ„ batch
- batch Ø£ØµØºØ±:
  - âœ… ÙŠØ­ØªØ§Ø¬ Ø°Ø§ÙƒØ±Ø© Ø£Ù‚Ù„
  - âŒ ØªØ¯Ø±ÙŠØ¨ Ø£Ø¨Ø·Ø£
- batch Ø£ÙƒØ¨Ø±:
  - âœ… ØªØ¯Ø±ÙŠØ¨ Ø£Ø³Ø±Ø¹
  - âŒ ÙŠØ­ØªØ§Ø¬ Ø°Ø§ÙƒØ±Ø© Ø£ÙƒØ«Ø±
- **16** ØªÙˆØ§Ø²Ù† Ø¬ÙŠØ¯ Ù„Ù€ DistilBERT

**shuffle=True** (Ù„Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙ‚Ø·):
```
Ù‚Ø¨Ù„ Ø§Ù„Ø®Ù„Ø·:
[angry, angry, happy, happy, sad, sad, ...]
         â†“
Ø¨Ø¹Ø¯ Ø§Ù„Ø®Ù„Ø·:
[happy, angry, sad, happy, angry, sad, ...]
```

**Ù„Ù…Ø§Ø°Ø§ Ø§Ù„Ø®Ù„Ø· Ù…Ù‡Ù…ØŸ**
- ÙŠÙ…Ù†Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† ØªØ¹Ù„Ù… ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
- ÙŠØ­Ø³Ù† Ø§Ù„ØªØ¹Ù…ÙŠÙ… (generalization)
- **Ù„Ø§ Ù†Ø®Ù„Ø·** dev/test Ù„Ø£Ù†Ù†Ø§ Ù†Ù‚ÙŠÙ‘Ù… ÙÙ‚Ø·

---

## ğŸ¤– Cell 10: Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ / Creating the Model

```python
device = "cuda" if torch.cuda.is_available() else "cpu"

model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=NUM_LABELS,
    problem_type="multi_label_classification"
)

model.to(device)
```

**Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ / Detailed Explanation:**

### 1. Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø¬Ù‡Ø§Ø² / Device Selection:
```python
device = "cuda" if torch.cuda.is_available() else "cpu"
```

- **cuda**: Ø¨Ø·Ø§Ù‚Ø© Ø±Ø³ÙˆÙ…Ø§Øª NVIDIA (GPU)
  - âœ… Ø£Ø³Ø±Ø¹ 10-100x
  - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…ÙˆØ§Ø²ÙŠØ©
  - Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„ØªØ¯Ø±ÙŠØ¨
  
- **cpu**: Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø¹Ø§Ø¯ÙŠ
  - âŒ Ø£Ø¨Ø·Ø£ Ø¨ÙƒØ«ÙŠØ±
  - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø°Ø§ÙƒØ±Ø© RAM
  - Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬ ÙÙ‚Ø·

### 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ / Loading Model:

**from_pretrained("distilbert-base-uncased")**:
- ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ù‹Ø§ (pre-trained)
- **Ù…Ø¯Ø±Ø¨ Ø¹Ù„Ù‰**: Wikipedia + BookCorpus
- **ÙŠØ¹Ø±Ù**: Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù„ØºØ©ØŒ Ù…Ø¹Ø§Ù†ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§ØªØŒ Ø§Ù„Ø³ÙŠØ§Ù‚

**Ù…Ø§ Ù‡Ùˆ Pre-trainingØŸ**
```
Phase 1: Pre-training (Ù…Ù„Ø§ÙŠÙŠÙ† Ø§Ù„Ù†ØµÙˆØµ)
â†“
ØªØ¹Ù„Ù… Ø§Ù„Ù„ØºØ© Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù…

Phase 2: Fine-tuning (Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§ØªÙ†Ø§)
â†“
ØªØ¹Ù„Ù… Ù…Ù‡Ù…Ø© Ù…Ø­Ø¯Ø¯Ø© (ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±)
```

**num_labels=28**:
- Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙˆØ§Ø·Ù Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù‡Ø§
- ÙŠØ¶ÙŠÙ Ø·Ø¨Ù‚Ø© classification Ø¨Ù€ 28 output

**problem_type="multi_label_classification"**:
- ÙŠØ®Ø¨Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ù†:
  - âœ… Ø¹Ø¯Ø© ØªØ³Ù…ÙŠØ§Øª Ù…Ù…ÙƒÙ†Ø© Ù„Ù†ÙØ³ Ø§Ù„Ù†Øµ
  - âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Sigmoid Ø¨Ø¯Ù„ Softmax
  - âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… BCE Loss Ø¨Ø¯Ù„ Cross-Entropy

### 3. Ù†Ù‚Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø§Ø² / Move to Device:
```python
model.to(device)
```
- Ù†Ù‚Ù„ Ø¬Ù…ÙŠØ¹ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ GPU
- Ø¶Ø±ÙˆØ±ÙŠ Ù‚Ø¨Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨

### Ù…Ø¹Ù…Ø§Ø±ÙŠØ© DistilBERT / DistilBERT Architecture:

```
Input Text: "I love this!"
     â†“
Tokenizer: [101, 1045, 2293, 2023, 999, 102]
     â†“
Embeddings (768 dimensions per token)
     â†“
Transformer Layers (6 layers)
â”œâ”€â”€ Multi-Head Attention
â”œâ”€â”€ Feed Forward Network
â”œâ”€â”€ Layer Normalization
â””â”€â”€ Residual Connections
     â†“
[CLS] Token Output (768 dimensions)
     â†“
Classification Head (768 â†’ 28)
     â†“
Output: [0.8, 0.1, 0.05, ..., 0.3]  # 28 probabilities
```

---

## âš™ï¸ Cell 11: Ø¥Ø¹Ø¯Ø§Ø¯ Optimizer Ùˆ Loss / Setting up Optimizer and Loss

```python
from torch.optim import AdamW
optimizer = AdamW(model.parameters(), lr=2e-5)

loss_fn = torch.nn.BCEWithLogitsLoss()
```

**Ø§Ù„Ø´Ø±Ø­ / Explanation:**

### 1. Optimizer: AdamW

**Ù…Ø§ Ù‡Ùˆ AdamWØŸ**
- Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù†Ø© Ù…Ù† Adam (Adaptive Moment Estimation)
- **W** = Weight Decay (ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ø£ÙˆØ²Ø§Ù†)
- Ø§Ù„Ø£ÙØ¶Ù„ Ù„Ù†Ù…Ø§Ø°Ø¬ Transformer

**ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ØŸ**
```
ÙÙŠ ÙƒÙ„ Ø®Ø·ÙˆØ© ØªØ¯Ø±ÙŠØ¨:
1. Ø­Ø³Ø§Ø¨ gradient (âˆ‚Loss/âˆ‚W)
2. Ø­Ø³Ø§Ø¨ moving average Ù„Ù„Ù€ gradient
3. Ø­Ø³Ø§Ø¨ moving average Ù„Ù„Ù€ gradientÂ²
4. ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… adaptive learning rate
```

**lr=2e-5 (0.00002)**:
- Ù…Ø¹Ø¯Ù„ ØªØ¹Ù„Ù… ØµØºÙŠØ± Ø¬Ø¯Ù‹Ø§
- Ù…Ù‡Ù… Ù„Ù€ Fine-tuning:
  - Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ù‹Ø§
  - Ù†Ø±ÙŠØ¯ ØªØ¹Ø¯ÙŠÙ„Ø§Øª ØµØºÙŠØ±Ø© ÙÙ‚Ø·
  - ØªØ¬Ù†Ø¨ "Ù†Ø³ÙŠØ§Ù†" Ù…Ø§ ØªØ¹Ù„Ù…Ù‡

**Ù…Ø¹Ø¯Ù„Ø§Øª ØªØ¹Ù„Ù… Ù†Ù…ÙˆØ°Ø¬ÙŠØ©:**
- Training from scratch: 0.001 - 0.01
- Fine-tuning BERT: 1e-5 - 5e-5
- Fine-tuning DistilBERT: 2e-5 - 3e-5

### 2. Loss Function: BCEWithLogitsLoss

**BCE** = Binary Cross-Entropy

**Ù„Ù…Ø§Ø°Ø§ BCEWithLogitsLossØŸ**
Ù„Ø£Ù†Ù†Ø§ ÙÙŠ multi-label classification:

```
Softmax (single-label):
[0.1, 0.7, 0.2] â†’ ÙŠØ¬Ø¨ Ø£Ù† ØªØ¬Ù…Ø¹ Ø¥Ù„Ù‰ 1
ÙÙ‚Ø· ÙˆØ§Ø­Ø¯ ØµØ­ÙŠØ­

Sigmoid (multi-label):
[0.8, 0.1, 0.9] â†’ ÙƒÙ„ ÙˆØ§Ø­Ø¯ Ù…Ø³ØªÙ‚Ù„
Ø¹Ø¯Ø© ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† ØµØ­ÙŠØ­Ø©
```

**Ø§Ù„ØµÙŠØºØ© Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©:**
```
BCE = -1/N Î£ [yÂ·log(Ïƒ(x)) + (1-y)Â·log(1-Ïƒ(x))]

Ø­ÙŠØ«:
- y: Ø§Ù„ØªØ³Ù…ÙŠØ© Ø§Ù„ÙØ¹Ù„ÙŠØ© (0 Ø£Ùˆ 1)
- Ïƒ(x): Sigmoid(x) = 1/(1+e^(-x))
- N: Ø¹Ø¯Ø¯ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª (28)
```

**WithLogits**:
- ÙŠØ·Ø¨Ù‚ Sigmoid Ø¯Ø§Ø®Ù„ÙŠÙ‹Ø§
- Ø£ÙƒØ«Ø± Ø§Ø³ØªÙ‚Ø±Ø§Ø±Ù‹Ø§ Ø¹Ø¯Ø¯ÙŠÙ‹Ø§
- Ø£Ø³Ø±Ø¹ ÙÙŠ Ø§Ù„Ø­Ø³Ø§Ø¨

**Ù…Ø«Ø§Ù„:**
```python
# Ø§Ù„ØªÙ†Ø¨Ø¤
predictions = [2.1, -0.5, 3.2]  # logits (Ù‚Ø¨Ù„ sigmoid)

# Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ©
labels = [1, 0, 1]

# BCEWithLogitsLoss ÙŠØ·Ø¨Ù‚:
1. Sigmoid: [0.89, 0.38, 0.96]
2. Ø­Ø³Ø§Ø¨ BCE: -[1Â·log(0.89) + 0Â·log(0.62) + 1Â·log(0.96)]
3. Ø§Ù„Ù†ØªÙŠØ¬Ø©: 0.15 (loss Ù…Ù†Ø®ÙØ¶ = Ø¬ÙŠØ¯)
```

---

## ğŸ‹ï¸ Cell 12: Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ / Training Loop

```python
from tqdm import tqdm

epochs = 2

for epoch in range(epochs):
    model.train()
    total_loss = 0

    for batch in tqdm(train_loader, desc=f"Training Epoch {epoch+1}"):
        input_ids, attention_masks, labels = [b.to(device) for b in batch]

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)
        
        loss = outputs.loss
        total_loss += loss.item()
        
        loss.backward()
        optimizer.step()

    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}")
```

**Ø§Ù„Ø´Ø±Ø­ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ / Detailed Explanation:**

### Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø© / Overall Structure:
```
for epoch in [1, 2]:
    for batch in train_loader:
        1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        2. Forward pass
        3. Ø­Ø³Ø§Ø¨ Loss
        4. Backward pass
        5. ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù†
```

### Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© / Step by Step:

**1. model.train()**:
```python
model.train()
```
- ØªÙØ¹ÙŠÙ„ ÙˆØ¶Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
- ÙŠÙØ¹Ù‘Ù„ Dropout Ùˆ BatchNormalization
- Ø¹ÙƒØ³ `model.eval()` Ù„Ù„ØªÙ‚ÙŠÙŠÙ…

**2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ GPU:**
```python
input_ids, attention_masks, labels = [b.to(device) for b in batch]
```
- Ù†Ù‚Ù„ ÙƒÙ„ batch Ø¥Ù„Ù‰ GPU
- Ø¶Ø±ÙˆØ±ÙŠ Ù„Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† GPU

**3. Ù…Ø³Ø­ Gradients Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:**
```python
optimizer.zero_grad()
```
- PyTorch ÙŠØ¬Ù…Ø¹ gradients
- ÙŠØ¬Ø¨ Ù…Ø³Ø­Ù‡Ø§ Ù‚Ø¨Ù„ ÙƒÙ„ backward pass

**4. Forward Pass:**
```python
outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)
```

**Ù…Ø§ ÙŠØ­Ø¯Ø« Ø¯Ø§Ø®Ù„ÙŠÙ‹Ø§:**
```
input_ids â†’ Embeddings
          â†“
    Transformer Layers (6x)
          â†“
    Classification Head
          â†“
    Logits (28 values)
          â†“
    Sigmoid + BCE Loss (Ù„Ø£Ù†Ù†Ø§ Ù…Ø±Ø±Ù†Ø§ labels)
          â†“
    outputs = {
        'loss': tensor(0.423),
        'logits': tensor([2.1, -0.5, ...])
    }
```

**5. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Loss:**
```python
loss = outputs.loss
total_loss += loss.item()
```
- `.loss`: Ù‚ÙŠÙ…Ø© Ø§Ù„Ø®Ø³Ø§Ø±Ø© (tensor)
- `.item()`: ØªØ­ÙˆÙŠÙ„ Ù…Ù† tensor Ø¥Ù„Ù‰ number

**6. Backward Pass (Ø­Ø³Ø§Ø¨ Gradients):**
```python
loss.backward()
```

**Ù…Ø§ ÙŠØ­Ø¯Ø«:**
```
Loss = 0.423
    â†“
Ø­Ø³Ø§Ø¨ âˆ‚Loss/âˆ‚W Ù„ÙƒÙ„ weight ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    â†“
ØªØ®Ø²ÙŠÙ† gradients ÙÙŠ W.grad
```

**7. ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù†:**
```python
optimizer.step()
```

**Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© AdamW:**
```python
for each weight W:
    W = W - lr * gradient
    (Ù…Ø¹ adaptive learning rate)
```

### Ù„Ù…Ø§Ø°Ø§ epochs=2 ÙÙ‚Ø·ØŸ

**Fine-tuning Ù†Ù…Ø§Ø°Ø¬ Pre-trained:**
- Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¹Ø±Ù Ø§Ù„Ù„ØºØ© Ø¨Ø§Ù„ÙØ¹Ù„
- Ù†Ø­ØªØ§Ø¬ ÙÙ‚Ø· ØªØ¹Ø¯ÙŠÙ„Ø§Øª ØµØºÙŠØ±Ø©
- **2-4 epochs** ÙƒØ§ÙÙŠØ© Ø¹Ø§Ø¯Ø©
- **Ø£ÙƒØ«Ø±** Ù‚Ø¯ ÙŠØ³Ø¨Ø¨ overfitting

**Training from scratch:**
- Ù‚Ø¯ Ù†Ø­ØªØ§Ø¬ 50-100 epoch
- Ù„Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¨Ø¯Ø£ Ù…Ù† Ø§Ù„ØµÙØ±

### tqdm:
```
Training Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2500/2500 [15:23<00:00, 2.71it/s]
```
- Ø´Ø±ÙŠØ· ØªÙ‚Ø¯Ù… Ø¬Ù…ÙŠÙ„
- ÙŠÙˆØ¶Ø­ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ
- Ø§Ù„Ø³Ø±Ø¹Ø© (iterations per second)

---

## ğŸ“Š Cell 13: Ø§Ù„ØªØ­Ù‚Ù‚ / Validation

```python
model.eval()
val_loss = 0

with torch.no_grad():
    for batch in tqdm(dev_loader, desc="Validation"):
        input_ids, attention_masks, labels = [b.to(device) for b in batch]

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_masks,
            labels=labels
        )
        
        val_loss += outputs.loss.item()

avg_val_loss = val_loss / len(dev_loader)
print(f"Validation Loss: {avg_val_loss:.4f}")
```

**Ø§Ù„Ø´Ø±Ø­ / Explanation:**

### 1. model.eval():
```python
model.eval()
```
- ØªÙØ¹ÙŠÙ„ ÙˆØ¶Ø¹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
- **ÙŠØ¹Ø·Ù‘Ù„**:
  - Dropout: Ø¥Ø¨Ù‚Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù€ neurons
  - BatchNorm: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø«Ø§Ø¨ØªØ©
- Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø£ÙƒØ«Ø± Ø§Ø³ØªÙ‚Ø±Ø§Ø±Ù‹Ø§

### 2. torch.no_grad():
```python
with torch.no_grad():
    # ... validation code ...
```

**Ø§Ù„ÙØ§Ø¦Ø¯Ø©:**
- Ù„Ø§ Ù†Ø­ØªØ§Ø¬ gradients Ù„Ù„ØªØ­Ù‚Ù‚
- ÙŠÙˆÙØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø© (~50%)
- ÙŠØ³Ø±Ø¹ Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª

**Ø§Ù„ÙØ±Ù‚:**
```python
# Ù…Ø¹ gradients (Ø§Ù„ØªØ¯Ø±ÙŠØ¨)
memory_used = 8 GB
time = 10 seconds

# Ø¨Ø¯ÙˆÙ† gradients (Ø§Ù„ØªØ­Ù‚Ù‚)
memory_used = 4 GB
time = 5 seconds
```

### 3. Ø­Ø³Ø§Ø¨ Validation Loss:

**Ù„Ù…Ø§Ø°Ø§ Ù…Ù‡Ù…ØŸ**
```
Training Loss    Validation Loss    Ø§Ù„ØªØ´Ø®ÙŠØµ
     â†“                 â†“
   0.2               0.25           âœ… Good (close)
   0.1               0.3            âš ï¸  Overfitting
   0.4               0.4            âš ï¸  Underfitting
```

**Overfitting**:
- Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
- Ù„Ø§ ÙŠØ¹Ù…Ù… Ø¬ÙŠØ¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©

**Underfitting**:
- Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù… ÙŠØªØ¹Ù„Ù… Ø¨Ø´ÙƒÙ„ ÙƒØ§ÙÙ
- Ø£Ø¯Ø§Ø¡ Ø³ÙŠØ¡ Ø¹Ù„Ù‰ ÙƒÙ„Ø§ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ØªÙŠÙ†

---

## ğŸ’¾ Cell 14: Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ / Saving the Model

```python
SAVE_PATH = "/kaggle/working/emotion_model"

model.save_pretrained(SAVE_PATH)
tokenizer.save_pretrained(SAVE_PATH)

print("Model Saved â†’", SAVE_PATH)
```

**Ø§Ù„Ø´Ø±Ø­ / Explanation:**

### Ù…Ø§ ÙŠØªÙ… Ø­ÙØ¸Ù‡ / What Gets Saved:

**model.save_pretrained()** ÙŠØ­ÙØ¸:
```
emotion_model/
â”œâ”€â”€ config.json           # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
â”œâ”€â”€ pytorch_model.bin     # Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø¯Ø±Ø¨Ø© (Ø£Ùˆ model.safetensors)
â””â”€â”€ special_tokens_map.json  # tokens Ø®Ø§ØµØ©
```

**tokenizer.save_pretrained()** ÙŠØ­ÙØ¸:
```
emotion_model/
â”œâ”€â”€ tokenizer_config.json  # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª tokenizer
â”œâ”€â”€ vocab.txt             # Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª (30,000+ ÙƒÙ„Ù…Ø©)
â””â”€â”€ tokenizer.json        # tokenizer ÙƒØ§Ù…Ù„
```

### Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„ÙØ§Øª / File Contents:

**config.json:**
```json
{
  "model_type": "distilbert",
  "num_labels": 28,
  "problem_type": "multi_label_classification",
  "vocab_size": 30522,
  "hidden_size": 768,
  "num_hidden_layers": 6,
  "num_attention_heads": 12,
  ...
}
```

**pytorch_model.bin:**
- Ù…Ù„Ù binary ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
- Ø§Ù„Ø­Ø¬Ù…: ~250 MB Ù„Ù€ DistilBERT
- ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„Ù‡ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ù€ `from_pretrained()`

### ÙƒÙŠÙÙŠØ© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù„Ø§Ø­Ù‚Ù‹Ø§ / How to Load Later:

```python
# ÙÙŠ app.py Ø£Ùˆ Ø£ÙŠ Ù…ÙƒØ§Ù† Ø¢Ø®Ø±
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification

model = DistilBertForSequenceClassification.from_pretrained("/path/to/emotion_model")
tokenizer = DistilBertTokenizerFast.from_pretrained("/path/to/emotion_model")

# Ø§Ù„Ø¢Ù† Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…!
text = "I love this!"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
```

---

## ğŸ“ Ù…Ù„Ø®Øµ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ / Training Process Summary

### ØªØ¯ÙÙ‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„ / Complete Data Flow:

```
1. Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… / Raw Data:
   "I love this movie!" â†’ GoEmotions dataset

2. Ø§Ù„ØªØ­Ø¶ÙŠØ± / Preprocessing:
   "I love this movie!" â†’ [101, 1045, 2293, 2023, 3185, 102]
   Labels: "17,18" â†’ [0,0,...,1,1,...,0] (28 dims)

3. Batching:
   16 Ù†Øµ â†’ Batch

4. Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ / Model:
   Batch â†’ DistilBERT â†’ Logits (16, 28)

5. Loss:
   Logits + Labels â†’ BCEWithLogitsLoss â†’ 0.423

6. Optimization:
   Loss â†’ Gradients â†’ Update Weights

7. Ø§Ù„ØªÙƒØ±Ø§Ø± / Repeat:
   Ø­ØªÙ‰ Ù†Ù‡Ø§ÙŠØ© Ø¬Ù…ÙŠØ¹ batches â†’ Epoch
   Ø­ØªÙ‰ Ù†Ù‡Ø§ÙŠØ© Ø¬Ù…ÙŠØ¹ epochs â†’ Training Complete

8. Ø§Ù„Ø­ÙØ¸ / Save:
   Model + Tokenizer â†’ Disk
```

---

## ğŸ¯ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù…Ù„ÙŠ / Practical Usage

### Ù…Ø«Ø§Ù„ ÙƒØ§Ù…Ù„ Ù„Ù„ØªÙ†Ø¨Ø¤ / Complete Prediction Example:

```python
import torch
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification

# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ / Load model
model = DistilBertForSequenceClassification.from_pretrained("./emotion_model")
tokenizer = DistilBertTokenizerFast.from_pretrained("./emotion_model")
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
model.eval()

# 2. Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ù„ÙŠÙ„Ù‡ / Text to analyze
text = "I'm so excited about this! But also a bit nervous..."

# 3. Tokenization
inputs = tokenizer(
    text,
    truncation=True,
    padding="max_length",
    max_length=128,
    return_tensors="pt"
)
input_ids = inputs["input_ids"].to(device)
attention_mask = inputs["attention_mask"].to(device)

# 4. Ø§Ù„ØªÙ†Ø¨Ø¤ / Prediction
with torch.no_grad():
    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
    logits = outputs.logits
    probabilities = torch.sigmoid(logits)

# 5. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹ÙˆØ§Ø·Ù / Extract emotions
emotion_names = [
    "admiration", "amusement", "anger", "annoyance", "approval", "caring",
    "confusion", "curiosity", "desire", "disappointment", "disapproval",
    "disgust", "embarrassment", "excitement", "fear", "gratitude", "grief",
    "joy", "love", "nervousness", "neutral", "optimism", "pride",
    "realization", "relief", "remorse", "sadness", "surprise"
]

probs = probabilities.cpu().numpy()[0]
threshold = 0.3

detected_emotions = []
for emotion, prob in zip(emotion_names, probs):
    if prob > threshold:
        detected_emotions.append({
            'emotion': emotion,
            'probability': round(prob * 100, 2)
        })

# 6. Ø§Ù„Ù†ØªØ§Ø¦Ø¬ / Results
detected_emotions.sort(key=lambda x: x['probability'], reverse=True)
print("Detected Emotions:")
for em in detected_emotions:
    print(f"  - {em['emotion']}: {em['probability']}%")

# Output:
# Detected Emotions:
#   - excitement: 87.5%
#   - nervousness: 65.2%
#   - joy: 45.3%
```

---

## ğŸ“Š Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØµÙˆØª / Comparison with Audio Model

| Feature | Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØµÙˆØª / Audio | Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Øµ / Text |
|---------|-------------------|------------------|
| **Ø§Ù„Ù†ÙˆØ¹** | CNN | Transformer (DistilBERT) |
| **Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª** | Ù…Ù„ÙØ§Øª ØµÙˆØªÙŠØ© | Ù†ØµÙˆØµ |
| **Ø§Ù„Ù…ÙŠØ²Ø§Øª** | ZCR, RMSE, MFCC | Tokens (ÙƒÙ„Ù…Ø§Øª ÙØ±Ø¹ÙŠØ©) |
| **Ø§Ù„Ø¹ÙˆØ§Ø·Ù** | 7 Ù…Ø´Ø§Ø¹Ø± | 28 Ø¹Ø§Ø·ÙØ© |
| **Classification** | Single-label | Multi-label |
| **Loss** | Categorical Cross-Entropy | BCE with Logits |
| **Framework** | TensorFlow/Keras | PyTorch |
| **Pre-training** | âŒ Ù…Ù† Ø§Ù„ØµÙØ± | âœ… DistilBERT |
| **Epochs** | 50 (Ù…Ø¹ early stopping) | 2 |
| **Ø§Ù„Ø­Ø¬Ù…** | ~50 MB | ~250 MB |
| **Ø§Ù„Ø³Ø±Ø¹Ø©** | Ø³Ø±ÙŠØ¹ | Ø£Ø¨Ø·Ø£ (Transformer) |
| **Ø§Ù„Ø¯Ù‚Ø©** | ~75-80% | ~70-75% (varies per emotion) |

---

## ğŸ” Ù†Ù‚Ø§Ø· Ù…Ù‡Ù…Ø© / Key Points

### 1. Multi-label vs Single-label:
```
Single-label (Audio):
ÙˆØ§Ø­Ø¯ ÙÙ‚Ø· ØµØ­ÙŠØ­
[0, 0, 1, 0, 0, 0, 0]

Multi-label (Text):
Ø¹Ø¯Ø© ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒÙˆÙ† ØµØ­ÙŠØ­Ø©
[1, 0, 0, 0, 1, 0, 1]
```

### 2. Transfer Learning:
```
Pre-training (Google):
100M+ texts â†’ ÙÙ‡Ù… Ø§Ù„Ù„ØºØ©

Fine-tuning (Ù†Ø­Ù†):
58K texts â†’ ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
```

### 3. Why DistilBERT?
- âœ… 40% Ø£Ø³Ø±Ø¹ Ù…Ù† BERT
- âœ… 60% Ø£ØµØºØ±
- âœ… 97% Ù…Ù† Ø£Ø¯Ø§Ø¡ BERT
- âœ… Ù…Ø«Ø§Ù„ÙŠ Ù„Ù„Ø¥Ù†ØªØ§Ø¬

### 4. Tokenization Magic:
```
"don't" â†’ ["don", "'", "t"]
"walking" â†’ ["walk", "##ing"]
"COVID-19" â†’ ["cov", "##id", "-", "19"]
```

### 5. Attention Mechanism:
```
Input: "I love this movie"
Attention: ÙƒÙ„ ÙƒÙ„Ù…Ø© ØªÙ†Ø¸Ø± Ø¥Ù„Ù‰ Ø¨Ø§Ù‚ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
"love" â†’ ÙŠÙ†ØªØ¨Ù‡ Ø¨Ø´Ø¯Ø© Ù„Ù€ "movie"
"this" â†’ ÙŠÙ†ØªØ¨Ù‡ Ø¨Ø´Ø¯Ø© Ù„Ù€ "movie"
```

---

## ğŸ’¡ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ù…ÙƒÙ†Ø© / Possible Improvements

### 1. Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù€ Epochs:
```python
epochs = 3  # Ø£Ùˆ 4
# Ù„ÙƒÙ† Ø§Ø­Ø°Ø± Ù…Ù† overfitting
```

### 2. Learning Rate Scheduling:
```python
from transformers import get_linear_schedule_with_warmup

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=len(train_loader) * epochs
)
```

### 3. Gradient Accumulation:
```python
# Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ø¹ batch size Ø£ÙƒØ¨Ø±
accumulation_steps = 4
for i, batch in enumerate(train_loader):
    loss = loss / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### 4. Early Stopping:
```python
best_val_loss = float('inf')
patience = 3
counter = 0

for epoch in range(epochs):
    train()
    val_loss = validate()
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        counter = 0
        save_model()
    else:
        counter += 1
        if counter >= patience:
            break
```

### 5. Data Augmentation:
```python
# Back-translation
"I love this" â†’ (Arabic) â†’ "I adore this"

# Synonym replacement
"happy" â†’ "joyful", "pleased", "delighted"

# Random deletion
"I love this movie" â†’ "I love movie"
```

---

## ğŸ“ Ø§Ù„Ø®Ù„Ø§ØµØ© / Conclusion

### Ù…Ø§ ØªØ¹Ù„Ù…Ù†Ø§ / What We Learned:

1. âœ… **Transformers**: Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ø«ÙˆØ±ÙŠØ© Ù„Ù„Ù€ NLP
2. âœ… **Transfer Learning**: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§
3. âœ… **Multi-label Classification**: ØªØµÙ†ÙŠÙ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
4. âœ… **Tokenization**: ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…
5. âœ… **PyTorch**: Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ Ù…Ø±Ù† Ù„Ù„Ù€ Deep Learning
6. âœ… **Fine-tuning**: ØªÙƒÙŠÙŠÙ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ù‡Ù…Ø© Ù…Ø­Ø¯Ø¯Ø©

### Ø§Ù„ÙØ±ÙˆÙ‚Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø¹Ù† Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØµÙˆØª / Key Differences from Audio Model:

| Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØµÙˆØª | Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Øµ |
|------------|-----------|
| Feature Engineering ÙŠØ¯ÙˆÙŠ | Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù…ÙŠØ²Ø§Øª |
| 7 Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ø­Ø¯Ø© | 28 Ø¹Ø§Ø·ÙØ© Ù…ØªØ¹Ø¯Ø¯Ø© |
| CNN Ø¨Ø³ÙŠØ· | Transformer Ù…Ø¹Ù‚Ø¯ |
| Ù…Ù† Ø§Ù„ØµÙØ± | Transfer Learning |
| 50 epochs | 2 epochs ÙÙ‚Ø· |

### Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ / Production Use:

Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ:
- âœ… ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ÙˆÙŠØ¨ (Flask/FastAPI)
- âœ… APIs
- âœ… ØªØ­Ù„ÙŠÙ„ ÙˆØ³Ø§Ø¦Ù„ Ø§Ù„ØªÙˆØ§ØµÙ„
- âœ… Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡
- âœ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª

---

**Ø§Ù†ØªÙ‡Ù‰ Ø´Ø±Ø­ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Øµ**
**End of Text Model Explanation**
